{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import keras\n",
    "from load_cifar import *\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"cifar-10-batches-py\"\n",
    "preprocess_data(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Hyper-perparmeter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size1 = 1024\n",
    "hidden_size2 = 1024\n",
    "output_size = 10\n",
    "lr = 0.0002\n",
    "epoch = 20\n",
    "batch_size = 32\n",
    "batch_num = 5\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"data\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "    training = tf.placeholder_with_default(False, shape=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNet(X):\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "    X_reshape = tf.layers.flatten(X)\n",
    "    fc1 = tf.layers.dense(X_reshape, hidden_size1, activation=tf.nn.relu, use_bias=True)#, kernel_regularizer=regularizer)\n",
    "    #layer1_drop = tf.layers.dropout(fc1, dropout_rate,  training=training)\n",
    "    fc2 =  tf.layers.dense(fc1, hidden_size2, activation=tf.nn.sigmoid, use_bias=True)#, kernel_regularizer=regularizer)\n",
    "    layer2_drop = tf.layers.dropout(fc2, dropout_rate,  training=training)\n",
    "    #fc3 =  tf.layers.dense(fc2, hidden_size3, activation=tf.nn.sigmoid, use_bias=True)\n",
    "    #fc4 =  tf.layers.dense(fc3, hidden_size3, activation=tf.nn.sigmoid, use_bias=True)\n",
    "    out =  tf.layers.dense(layer2_drop, output_size, activation=tf.nn.softmax, use_bias=True)\n",
    "    return out\n",
    "out = neuralNet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define cost andoptimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out)\n",
    "correct_pred = tf.equal(tf.argmax(out,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and testing</h1>\n",
    "<h2>1.Print out validation accuracy after each training poch</h2>\n",
    "<h2>2.Print out training time you spend on each epoch</h2>\n",
    "<h2>3.Print out testing accuracy in the end</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, Training accuracy= 0.333\n",
      "epoch 0, Validation accuracy= 0.347\n",
      "time elapsed at 0th epoch is:  9  seconds\n",
      "epoch 1, Training accuracy= 0.365\n",
      "epoch 1, Validation accuracy= 0.345\n",
      "time elapsed at 1th epoch is:  19  seconds\n",
      "epoch 2, Training accuracy= 0.385\n",
      "epoch 2, Validation accuracy= 0.389\n",
      "time elapsed at 2th epoch is:  28  seconds\n",
      "epoch 3, Training accuracy= 0.408\n",
      "epoch 3, Validation accuracy= 0.391\n",
      "time elapsed at 3th epoch is:  37  seconds\n",
      "epoch 4, Training accuracy= 0.422\n",
      "epoch 4, Validation accuracy= 0.417\n",
      "time elapsed at 4th epoch is:  46  seconds\n",
      "epoch 5, Training accuracy= 0.436\n",
      "epoch 5, Validation accuracy= 0.423\n",
      "time elapsed at 5th epoch is:  55  seconds\n",
      "epoch 6, Training accuracy= 0.444\n",
      "epoch 6, Validation accuracy= 0.431\n",
      "time elapsed at 6th epoch is:  64  seconds\n",
      "epoch 7, Training accuracy= 0.452\n",
      "epoch 7, Validation accuracy= 0.434\n",
      "time elapsed at 7th epoch is:  73  seconds\n",
      "epoch 8, Training accuracy= 0.465\n",
      "epoch 8, Validation accuracy= 0.446\n",
      "time elapsed at 8th epoch is:  82  seconds\n",
      "epoch 9, Training accuracy= 0.471\n",
      "epoch 9, Validation accuracy= 0.438\n",
      "time elapsed at 9th epoch is:  91  seconds\n",
      "epoch 10, Training accuracy= 0.478\n",
      "epoch 10, Validation accuracy= 0.438\n",
      "time elapsed at 10th epoch is:  100  seconds\n",
      "epoch 11, Training accuracy= 0.483\n",
      "epoch 11, Validation accuracy= 0.436\n",
      "time elapsed at 11th epoch is:  109  seconds\n",
      "epoch 12, Training accuracy= 0.493\n",
      "epoch 12, Validation accuracy= 0.452\n",
      "time elapsed at 12th epoch is:  118  seconds\n",
      "epoch 13, Training accuracy= 0.502\n",
      "epoch 13, Validation accuracy= 0.456\n",
      "time elapsed at 13th epoch is:  127  seconds\n",
      "epoch 14, Training accuracy= 0.503\n",
      "epoch 14, Validation accuracy= 0.463\n",
      "time elapsed at 14th epoch is:  136  seconds\n",
      "epoch 15, Training accuracy= 0.512\n",
      "epoch 15, Validation accuracy= 0.462\n",
      "time elapsed at 15th epoch is:  144  seconds\n",
      "epoch 16, Training accuracy= 0.519\n",
      "epoch 16, Validation accuracy= 0.464\n",
      "time elapsed at 16th epoch is:  153  seconds\n",
      "epoch 17, Training accuracy= 0.524\n",
      "epoch 17, Validation accuracy= 0.463\n",
      "time elapsed at 17th epoch is:  162  seconds\n",
      "epoch 18, Training accuracy= 0.526\n",
      "epoch 18, Validation accuracy= 0.477\n",
      "time elapsed at 18th epoch is:  170  seconds\n",
      "epoch 19, Training accuracy= 0.531\n",
      "epoch 19, Validation accuracy= 0.466\n",
      "time elapsed at 19th epoch is:  179  seconds\n",
      "Training finished!\n",
      "Test accuracy is: 0.476\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch):\n",
    "        for j in range(1, batch_num + 1):\n",
    "            file_name = 'preprocess_batch_' + str(j) + '.p'\n",
    "            data = pickle.load(open(file_name, 'rb'))\n",
    "            features, labels = data[0], data[1]\n",
    "            tr_acc = 0\n",
    "            count = 0\n",
    "            for batch_x, batch_y in mini_batch(features, labels, batch_size):\n",
    "                sess.run(train_op, feed_dict={X:batch_x, y:batch_y, training: True})\n",
    "                if j == batch_num:\n",
    "                    tr_acc += sess.run(accuracy,feed_dict={X:batch_x, y:batch_y})\n",
    "                    count += 1\n",
    "            if j == batch_num:\n",
    "                print(\"epoch \"+str(i)+\", Training accuracy= {:.3f}\".format(tr_acc / count))\n",
    "        val_x, val_y = load_preprocessed_validation_batch()\n",
    "        val_acc = sess.run(accuracy,feed_dict={X:val_x, y:val_y})\n",
    "        print(\"epoch \"+str(i)+\", Validation accuracy= {:.3f}\".format(val_acc))\n",
    "        print(\"time elapsed at \"+str(i)+\"th epoch is: \", int(time.time() - start), \" seconds\")\n",
    "    save_path = saver.save(sess, 'model.ckpt')\n",
    "    print(\"Training finished!\")\n",
    "    acc_test = 0.0\n",
    "    count = 0\n",
    "    test_batch_size = 200\n",
    "    for batch_x, batch_y in load_preprocessed_test_batch(test_batch_size):\n",
    "            test_acc = sess.run(accuracy, feed_dict={X:batch_x, y:batch_y})\n",
    "            acc_test += test_acc\n",
    "            count += 1\n",
    "    print(\"Test accuracy is: {:.3f}\".format(acc_test / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, I split training set and validation set using the ratio(98:2) from the original training set. For the fully connected neural network. I first explore the shallow layers of 2, and search hidden size between 200 and 2000. I also try to increase the layer to 3 and even more, but for me I don't get better accuracy. The best accuracy I've got so far is using 2 layers with both layers the hidden size of 1024. After training 20 epoches, I get test accuracy of 0.476 for my test set. I try to ask others how they get an accuracy of 50% within 10 epoches and try their hyperparameter settings, but the parameter doesn't work for me. I believe there's some problem with my data preprocessing in part(a) that causes the inadequate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
